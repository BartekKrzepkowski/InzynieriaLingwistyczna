{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "1zRHrti6bdHZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Zadanie\n",
        "\n",
        "1. Jakie najlepsze wagi byśmy dali i jakie są najlepsze możliwe wyniki\n",
        "2. Popraw działanie sieci manipulując lr i epoch\n",
        "3. Przyspiesz trening dodając \"neurony\""
      ]
    },
    {
      "metadata": {
        "id": "WyWscXWCbYqh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "1f273ab2-f360-4040-9c44-109cfae8bac3"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# N is batch size; \n",
        "# D_in is input dimension;\n",
        "# D_out is output dimension.\n",
        "\n",
        "N, D_in, D_out = 4, 2, 1\n",
        "\n",
        "x=np.array(    [[0., 0.],\n",
        "                [0., 1.],\n",
        "                [1., 0.],\n",
        "                [1., 1.]])\n",
        "          \n",
        "print(x)\n",
        "\n",
        "y=np.array(     [[0.],\n",
        "                 [0.],\n",
        "                 [0.],\n",
        "                 [1.]])\n",
        "\n",
        "print(y)\n",
        "\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = np.random.randn(D_in, D_out)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "for t in range(epochs):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = x.dot(w1)\n",
        "    \n",
        "    # Compute and print loss\n",
        "    loss = np.square(y_pred - y).sum()\n",
        "    print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of w1 with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_w1 = x.T.dot(grad_y_pred)\n",
        "\n",
        "    # Update weights\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    \n",
        "y_test = x.dot(w1)\n",
        "print(\"Results:\")\n",
        "print(y_test)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 1.]]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]]\n",
            "0 3.6166164792592355\n",
            "1 3.6165910045880354\n",
            "2 3.6165655301668322\n",
            "3 3.616540055995623\n",
            "4 3.6165145820744042\n",
            "5 3.616489108403175\n",
            "6 3.616463634981931\n",
            "7 3.6164381618106693\n",
            "8 3.6164126888893886\n",
            "9 3.6163872162180843\n",
            "Results:\n",
            "[[ 0.        ]\n",
            " [ 0.7591725 ]\n",
            " [-1.10657849]\n",
            " [-0.34740599]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}