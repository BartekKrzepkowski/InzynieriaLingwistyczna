{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "1zRHrti6bdHZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Zadanie\n",
        "\n",
        "1. Jakie najlepsze wagi byśmy dali i jakie są najlepsze możliwe wyniki\n",
        "2. Popraw działanie sieci manipulując lr i epoch\n",
        "3. Przyspiesz trening dodając \"neurony\""
      ]
    },
    {
      "metadata": {
        "id": "WyWscXWCbYqh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# N is batch size; \n",
        "# D_in is input dimension;\n",
        "# D_out is output dimension.\n",
        "\n",
        "N, D_in, D_out = 4, 2, 1\n",
        "\n",
        "x=np.array(    [[0., 0.],\n",
        "                [0., 1.],\n",
        "                [1., 0.],\n",
        "                [1., 1.]])\n",
        "          \n",
        "print(x)\n",
        "\n",
        "y=np.array(     [[0.],\n",
        "                 [0.],\n",
        "                 [0.],\n",
        "                 [1.]])\n",
        "\n",
        "print(y)\n",
        "\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = np.random.randn(D_in, D_out)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "for t in range(epochs):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = x.dot(w1)\n",
        "    \n",
        "    # Compute and print loss\n",
        "    loss = np.square(y_pred - y).sum()\n",
        "    print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of w1 with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_w1 = x.T.dot(grad_y_pred)\n",
        "\n",
        "    # Update weights\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    \n",
        "y_test = x.dot(w1)\n",
        "print(\"Results:\")\n",
        "print(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}